{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticToolsParser, PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from typing import Sequence, List\n",
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"Context_files\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    print()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system1 = context_gen(\"system1.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "refsystem = context_gen(\"ref_system copy.txt\")\n",
    "\n",
    "system3 = context_gen(\"system3.txt\")\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "\n",
    "rag_prompt = context_gen(\"rag_prompt.txt\")\n",
    "\n",
    "\n",
    "#First input to the model, breaks query down and provides geometry definition\n",
    "\n",
    "class analyzed_query(BaseModel):\n",
    "    \"\"\"Identifying whether a geometric feature is present in the database and stating the name of the feature\"\"\"\n",
    "    answer: str= Field(description=\"YES or NO\")\n",
    "    features: List= Field(description=\"Name of the identified geometrical feature (Singular)\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Listing the answer of whether the feature is present in the database and the name of the feature\"\"\"\n",
    "    Response: List[analyzed_query]\n",
    "    \n",
    "#chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "#chat = ChatVertexAI(model_name=\"gemini-1.0-pro\", temperature=0.0, convert_system_message_to_human=True)\n",
    "chat = ChatAnthropic(model_name = \"claude-3-sonnet-20240229\", temperature=0.0)\n",
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select all faces in the xy plane that have a radius greater than 5mm\n",
      "---\n",
      "\n",
      "<- filter:>\n",
      "select * from faces where radius > 5 and normal_zangle between 89 and 91\n",
      "---\n",
      "\n",
      "The provided YAML code looks good for selecting all faces in the XY plane that have a radius greater than 5mm:\n",
      "\n",
      "- filter: select * from faces where radius > 5 and normal_zangle between 89 and 91\n",
      "\n",
      "A few observations:\n",
      "\n",
      "1. The query correctly filters the `faces` table.\n",
      "2. It uses the `radius` column to select faces with radius greater than 5. \n",
      "3. The `normal_zangle` condition between 89 and 91 degrees effectively selects faces lying in the XY plane.\n",
      "\n",
      "This YAML adheres to the specified format of starting with `- filter:` and seems to accurately translate the natural language query. No additional feedback or improvements are needed.\n",
      "---\n",
      "\n",
      "Thank you for the feedback. I'm glad the generated YAML code accurately translates the natural language query and adheres to the specified format. I will continue to follow the same approach for future queries.\n",
      "---\n",
      "\n",
      "You're welcome! I'm glad I could provide helpful feedback on the generated YAML code. Continuing to follow the same approach of accurately translating natural language queries into the specified YAML format will be beneficial. Feel free to share any other examples or queries, and I'll be happy to review and provide feedback on the corresponding YAML code generation. Maintaining consistency and adhering to the specified format is crucial for effective code generation and review.\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''change the human variable to test different inputs'''\n",
    "human = (\"Select all faces in the xy plane that have a radius greater than 5mm\")\n",
    "\n",
    "base_prompt = ChatPromptTemplate.from_messages([(\"system\", system1), MessagesPlaceholder(variable_name=\"messages\")])\n",
    "yaml_generator = base_prompt | chat \n",
    "\n",
    "#This chain ise used for the reflection prompt, which asks the model to critique and improve on the generated YAML text.\n",
    "reflection_prompt = ChatPromptTemplate.from_messages([(\"system\", refsystem), few_shot_prompt, MessagesPlaceholder(variable_name=\"messages\")])\n",
    "reflect = reflection_prompt | chat \n",
    "\"\"\"\n",
    "final_output = reflect.invoke({\"messages\": [HumanMessage(content=str(output))]})\n",
    "print(final_output.content)\n",
    "\"\"\"\n",
    "async def generation_node(state: Sequence[BaseMessage]):\n",
    "        return await yaml_generator.ainvoke({\"messages\": state}, {\"tags\": [\"loop 004\"]})\n",
    "\n",
    "async def reflection_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "        # Other messages we need to adjust\n",
    "        cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "        # First message is the original user request. We hold it the same for all nodes\n",
    "        translated = [state[0]] + [cls_map[msg.type](content=msg.content) for msg in state[1:]]\n",
    "        res = await reflect.ainvoke({\"messages\": translated})\n",
    "        # We treat the output of this as human feedback for the generator\n",
    "        return HumanMessage(content=res.content)\n",
    "\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.set_entry_point(\"generate\")\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > 3:\n",
    "        return END\n",
    "    return \"generate\"\n",
    "\n",
    "builder.add_conditional_edges(\"reflect\", should_continue)\n",
    "builder.add_edge(\"generate\", \"reflect\")\n",
    "graph = builder.compile()\n",
    "\n",
    "async for event in graph.astream([HumanMessage(content=human)]):\n",
    "        for key, value in event.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(value.get('content'))\n",
    "                print('---')\n",
    "                print()\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        print(item.get('content'))\n",
    "                        print('---')\n",
    "                        print()\n",
    "                    else:\n",
    "                        print(item.content)\n",
    "                        print('---')\n",
    "                        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
