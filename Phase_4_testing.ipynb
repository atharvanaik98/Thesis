{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticToolsParser, PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from typing import Sequence, List\n",
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"Context_files\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    print()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system1 = context_gen(\"system1.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "refsystem = context_gen(\"ref_system copy.txt\")\n",
    "\n",
    "system3 = context_gen(\"system3.txt\")\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "\n",
    "rag_prompt = context_gen(\"rag_prompt.txt\")\n",
    "\n",
    "\n",
    "#First input to the model, breaks query down and provides geometry definition\n",
    "\n",
    "class analyzed_query(BaseModel):\n",
    "    \"\"\"Identifying whether a geometric feature is present in the database and stating the name of the feature\"\"\"\n",
    "    answer: str= Field(description=\"YES or NO\")\n",
    "    features: List= Field(description=\"Name of the identified geometrical feature (Singular)\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Listing the answer of whether the feature is present in the database and the name of the feature\"\"\"\n",
    "    Response: List[analyzed_query]\n",
    "    \n",
    "\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chat_google = ChatVertexAI(model_name=\"gemini-1.0-pro\", temperature=0.0, convert_system_message_to_ai=True)\n",
    "chat_claude = ChatAnthropic(model_name = \"claude-3-sonnet-20240229\", temperature=0.0)\n",
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''change the human variable to test different inputs'''\n",
    "human = (\"select all leading edges\")\n",
    "\n",
    "def generate_yaml(human:str, chat):\n",
    "    parser = PydanticOutputParser(pydantic_object=analyzed_query)\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", rag_prompt + \"Answer the user query. Print the output in Json format according to the instructions.\\n{format_instructions}\",), (\"human\", human),]).partial(format_instructions=parser.get_format_instructions())\n",
    "    chain = prompt | chat | parser\n",
    "    feature_extractor = chain.invoke({})\n",
    "    answer = feature_extractor.answer\n",
    "\n",
    "    extracted_features = feature_extractor.features\n",
    "    print(feature_extractor)\n",
    "    print()\n",
    "    loader = DirectoryLoader(\"RAG\")\n",
    "    raw_docs = loader.load()\n",
    "    textsplitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "    docs = textsplitter.split_documents(raw_docs)\n",
    "    db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "\n",
    "    prompt_for_rag = ChatPromptTemplate.from_template(\"\"\"You are an assistant that is an expert at RAG (Retrieval Augmented Generation). You have been tasked with looking for the definition of the word provided to you as input and generate a summary of the available information.\"\n",
    "                You will use only the context provided to you.                                 \n",
    "                Context:{context}\n",
    "                Query_to_db:{query}\"\"\")\n",
    "    rag_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()}) | prompt_for_rag | chat | StrOutputParser()\n",
    "    #Base prompt chain, generates the first yaml version with minimal context\n",
    "\n",
    "    base_prompt = ChatPromptTemplate.from_messages([(\"system\", system1), few_shot_prompt, MessagesPlaceholder(variable_name=\"messages\")])\n",
    "    yaml_generator = base_prompt | chat \n",
    "    rag_output = rag_chain.invoke(f\"What is the definition of {extracted_features}?\")\n",
    "\n",
    "    print(rag_output)\n",
    "    print()\n",
    "\n",
    "    def chain_yes():\n",
    "        chain = yaml_generator | StrOutputParser()\n",
    "        yaml = chain.invoke({\"messages\": [human]}, {\"tags\": [\"chain_yes\"]})\n",
    "        print(\"RAG not required, generating YAML\")\n",
    "        print()\n",
    "        print(yaml)\n",
    "        print()\n",
    "        return yaml\n",
    "\n",
    "    def chain_no():\n",
    "        chain = yaml_generator | StrOutputParser()\n",
    "        ai = rag_output\n",
    "        yaml = chain.invoke({\"messages\":[ai]}, {\"tags\": [\"chain_no\"]})\n",
    "        print(\"Getting more information using RAG, then generating YAML\")\n",
    "        print()\n",
    "        print(yaml)\n",
    "        print()\n",
    "        return yaml\n",
    "\n",
    "    def decision_maker(response_obj: analyzed_query):\n",
    "        for item in response_obj:\n",
    "            if item == \"YES\":\n",
    "                return chain_yes()\n",
    "            elif item == \"NO\":\n",
    "                return chain_no()\n",
    "\n",
    "    model_output = []\n",
    "    for answer in feature_extractor:\n",
    "        model_output.append(decision_maker(answer))\n",
    "    #This chain ise used for the reflection prompt, which asks the model to critique and improve on the generated YAML text.\n",
    "    reflection_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", refsystem),\n",
    "            (\"user\", rag_output),\n",
    "            few_shot_prompt,\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "    reflect = reflection_prompt | chat | StrOutputParser()\n",
    "    message = str(f\"This is the YAML code you are supposed to provide feedback for: {model_output[0]}.\" + f\"The user query for which the yaml code provided is: \\n{human}\")\n",
    "    feedback = reflect.invoke(\n",
    "        {\"messages\": [message]}, {\"tags\": [\"feedback\"]})\n",
    "    final_generate = base_prompt | chat | StrOutputParser()\n",
    "    output_yaml = final_generate.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                f\"I have some YAML code \\n {model_output[0]}\"\n",
    "                + f\"\\nThis code needs to be regenerated (only if needed) with the following instructions in mind: \\n{feedback}. \\nThe YAML code generated needs to be for the following query: {human}\"\n",
    "            ]\n",
    "        },\n",
    "        {\"tags\": [\"final_generate\"]},\n",
    "        )\n",
    "    print(output_yaml)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='NO' features=['flanges']\n",
      "\n",
      "## Definition of Flanges:\n",
      "\n",
      "Based on the provided context, here's the definition of flanges:\n",
      "\n",
      "**Flanges are planar surfaces with a relatively large area compared to neighboring surfaces.** They are typically used in conjunction with other components, such as pipes or beams, to provide a secure connection. \n",
      "\n",
      "Here are some key characteristics of flanges:\n",
      "\n",
      "* **Large area:** Compared to neighboring surfaces, flanges have a significantly larger surface area. This allows them to distribute forces more evenly and provide a stronger connection.\n",
      "* **Planar:** Flanges are flat surfaces, which makes them easy to machine and assemble.\n",
      "* **Used for connections:** Flanges are often used to connect pipes, beams, and other components together. They can be bolted, welded, or glued in place.\n",
      "\n",
      "Here are some additional details about flanges:\n",
      "\n",
      "* **Identification in the faces table:** Flanges can be identified in the faces table by their large area and planar type.\n",
      "* **SQL query for selection:** The provided context includes an SQL query that can be used to select flanges based on specific criteria.\n",
      "\n",
      "I hope this information is helpful! Let me know if you have any other questions.\n",
      "\n",
      "Getting more information using RAG, then generating YAML\n",
      "\n",
      "## YAML query for selecting flanges:\n",
      "\n",
      "Based on the provided definition of flanges, here's a YAML query that can be used to select them from the faces table:\n",
      "\n",
      "```yaml\n",
      "- filter: select * from faces where area > 100 and type like \"Planar\"\n",
      "```\n",
      "\n",
      "This query selects all faces with an area greater than 100 square units and a type of \"Planar\". These criteria are consistent with the definition of flanges as having a large area and being planar.\n",
      "\n",
      "**Note:** The specific area threshold of 100 square units is arbitrary and can be adjusted based on the specific context and requirements.\n",
      "\n",
      "Here are some additional considerations for selecting flanges:\n",
      "\n",
      "* **Material:** The material of the flange can be specified in the query if it is relevant.\n",
      "* **Thickness:** The thickness of the flange can also be specified if it is important.\n",
      "* **Location:** The location of the flange can be specified using spatial filters.\n",
      "\n",
      "I hope this YAML query is helpful for selecting flanges from the faces table. Please let me know if you have any other questions.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised InvalidArgument: 400 Please ensure that multiturn requests alternate between user and model..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised InvalidArgument: 400 Please ensure that multiturn requests alternate between user and model..\n"
     ]
    }
   ],
   "source": [
    "human = \"Select all flanges in the model\"\n",
    "generate_yaml(human, chat_google)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
