{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"VertexLang\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "#invoke and run the model with the given prompt\n",
    "def test_vertexai(few_shot_prompt):\n",
    "    generate = ChatPromptTemplate.from_messages([(\"system\", system), few_shot_prompt, (\"human\", human),])\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    #select an output parser\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = generate | chat | output_parser\n",
    "    result = chain.invoke({}, {\"tags\": [\"loop 001\"]})\n",
    "    print(remove_code_fences(result))\n",
    "    print()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system = context_gen(\"systemmsg.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "'''change the human variable to test different inputs'''\n",
    "human = (\"select the blend in the xz plane between the angle 45 and 90 and then select the connected blend\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQuery(BaseModel):\n",
    "    '''Search for the geometric definition of a feature across the internet'''\n",
    "\n",
    "    sub_query: str = Field(\n",
    "        ...,\n",
    "        description=\"The text to be used as a sub-query in the prompt.\",\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SubQuery(sub_query='What is the geometric definition of a blend in the xz plane?'),\n",
       " SubQuery(sub_query='What is the geometric definition of connected blends in a geometric context?')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid \n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain_core.messages import (AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage)\n",
    "\n",
    "def tool_example_to_message(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\"id\": str(uuid.uuid4()), \"type\": \"function\", \n",
    "             \"function\": {\n",
    "                 \"name\": tool_call.__class__.__name__,\n",
    "                 \"arguments\": tool_call.json(),                    \n",
    "                        },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "    AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    ")\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"This is an example of a correct usage of this tool. Make sure to continue using the tool this way.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "examples = []\n",
    "user = \"Select all boltholes\"\n",
    "queries = [\n",
    "    SubQuery(sub_query=\"What is the definition of boltholes\"),\n",
    "]\n",
    "examples.append({\"input\": user, \"tool_calls\": queries})\n",
    "\n",
    "user = \"select all vanes\"\n",
    "queries = [\n",
    "    SubQuery(sub_query=\"What is the definition of vanes\"),\n",
    "]\n",
    "examples.append({\"input\": user, \"tool_calls\": queries})\n",
    "\n",
    "example_messages = [msg for ex in examples for msg in tool_example_to_message(ex)]\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system2), MessagesPlaceholder(\"examples\", optional=True), (\"human\", human),])\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "chat_with_tools = chat.bind_tools([SubQuery])\n",
    "parser = PydanticToolsParser(tools = [SubQuery])\n",
    "query_analyzer = prompt | chat_with_tools | parser \n",
    "query_analyzer.invoke({}, {\"tags\": [\"loop 001\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot prompts example template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- filter: select * from faces where type like \"Blend\" and (xyangle between 45 and 90)\n",
      "- expand: connectedblend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "'''Function call to run the model'''\n",
    "test_vertexai(few_shot_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
