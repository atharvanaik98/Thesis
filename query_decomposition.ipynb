{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from typing import Sequence, List\n",
    "from langgraph.graph import MessageGraph, END\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"Context_files\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    print()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "#invoke and run the model with the given prompt\n",
    "def test_vertexai(few_shot_prompt):\n",
    "    generate = ChatPromptTemplate.from_messages([(\"system\", system), few_shot_prompt, (\"human\", human), (\"ai\", someoutput)])\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    #select an output parser\n",
    "    output_parser = StrOutputParser()\n",
    "    chain = generate | chat | output_parser\n",
    "    result = chain.invoke({})\n",
    "    print(remove_code_fences(result))\n",
    "    print()\n",
    "    return\n",
    "\n",
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system1 = context_gen(\"system1.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "refsystem = context_gen(\"ref_system copy.txt\")\n",
    "\n",
    "system3 = context_gen(\"system3.txt\")\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "\n",
    "class SubQuery(BaseModel):\n",
    "    '''Search for the geometric definition of a feature across the vector database'''\n",
    "\n",
    "    sub_query: str = Field(description=\"The text to be used as a sub-query in the prompt.\")   \n",
    "#First input to the model, breaks query down and provides geometry definition\n",
    "\n",
    "class YamlText(BaseModel):\n",
    "    text: str = Field(SQLQuery = \"Yaml Text printed one line at a time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SubQuery(sub_query='What is the definition of radius?')]\n"
     ]
    }
   ],
   "source": [
    "#This model sends the user query to the LLM, which then breaks it down into smaller parts if there are words it does not understand\"\"\"\n",
    "\n",
    "'''change the human variable to test different inputs'''\n",
    "human = (\"select all faces with a radius less than 48.5mm and then select their edges\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system2), (\"human\", human),])\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5, )\n",
    "chat_with_tools = chat.bind_tools([SubQuery])\n",
    "parser = PydanticToolsParser(tools=[SubQuery])\n",
    "query_analyzer = prompt | chat_with_tools | parser \n",
    "output = query_analyzer.invoke({}, {\"tags\": [\"loop 001\"]})\n",
    "\n",
    "\n",
    "strings = str([item for item in output])\n",
    "print(strings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A radius, in terms of CAD geometrical features, can be defined as a characteristic length measurement from the center to the outer edge of a planar or curved surface, such as a face or an edge. The radius can be major or minor, depending on the orientation of the surface normal, and it is a key parameter for defining the shape and curvature of the feature.\n"
     ]
    }
   ],
   "source": [
    "#chat prompt template for defining the words \n",
    "definer = ChatPromptTemplate.from_messages([(\"system\", system3), (\"human\", strings),])\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, streaming=True)\n",
    "\n",
    "#using string output parser as the output is a string\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = definer | chat \n",
    "definition = chain2.invoke({}, {\"tags\":[\"loop 002\"]})\n",
    "print(definition.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base prompt chain, generates the first yaml version with minimal context\n",
    "base_prompt = ChatPromptTemplate.from_messages([(\"system\", system1), MessagesPlaceholder(variable_name=\"messages\")])\n",
    "base_chain = base_prompt | chat \n",
    "\n",
    "#This chain ise used for the reflection prompt, which asks the model to critique and improve on the generated YAML text.\n",
    "reflection_prompt = ChatPromptTemplate.from_messages([(\"system\", refsystem), few_shot_prompt, (\"ai\", definition.content), MessagesPlaceholder(variable_name=\"messages\"), ])\n",
    "reflect = reflection_prompt | chat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select all faces with a radius less than 48.5mm and then select their edges\n",
      "---\n",
      "\n",
      "- filter: select * from faces where radius < 48.5\n",
      "- expand: faceedges\n",
      "---\n",
      "\n",
      "The provided YAML code is accurate and meets the requirements specified in the user query. It correctly selects all faces with a radius less than 48.5mm and then proceeds to select their corresponding edges. Well done!\n",
      "---\n",
      "\n",
      "Thank you! If you have any more queries or need further assistance, feel free to ask.\n",
      "---\n",
      "\n",
      "Thank you! If you have any more queries or need further assistance, feel free to ask. Have a great day!\n",
      "---\n",
      "\n",
      "You're welcome! Have a great day too!\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def generation_node(state: Sequence[BaseMessage]):\n",
    "    return await base_chain.ainvoke({\"messages\": state}, {\"tags\": [\"loop 003\"]})\n",
    "\n",
    "\n",
    "async def reflection_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    # Other messages we need to adjust\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    # First message is the original user request. We hold it the same for all nodes\n",
    "    translated = [state[0]] + [cls_map[msg.type](content=msg.content) for msg in state[1:]]\n",
    "    res = await reflect.ainvoke({\"messages\": translated})\n",
    "    # We treat the output of this as human feedback for the generator\n",
    "    return HumanMessage(content=res.content)\n",
    "\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.set_entry_point(\"generate\")\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > 5:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "graph = builder.compile()\n",
    "\n",
    "async for event in graph.astream([HumanMessage(content=human)]):\n",
    "    for key, value in event.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(value.get('content'))\n",
    "            print('---')\n",
    "            print()\n",
    "        elif isinstance(value, list):\n",
    "             for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    print(item.get('content'))\n",
    "                    print('---')\n",
    "                    print()\n",
    "                else:\n",
    "                    print(item.content)\n",
    "                    print('---')\n",
    "                    print()\n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
