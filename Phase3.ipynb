{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import uuid\n",
    "\n",
    "import nest_asyncio\n",
    "import vertexai\n",
    "nest_asyncio.apply()\n",
    "vertexai.init(project=\"simcenter-llm-trial\")\n",
    "\n",
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"Context_files\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    print()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system1 = context_gen(\"system1.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "refsystem = context_gen(\"ref_system copy.txt\")\n",
    "\n",
    "system3 = context_gen(\"system3.txt\")\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "\n",
    "qa_instructions = context_gen(\"qa_instructions.txt\")\n",
    "\n",
    "a = \"Select all faces with radius less than 10\"\n",
    "b = \"Select all faces with radius less than 5 and then select their corresponding edges\"\n",
    "c = \"Select circular edges with a radius less than 6 and then select their adjacent edges\"\n",
    "d = \"Select all faces in the lower half of the model that have a radius less than 10 that are blends\"\n",
    "e = \"Select all faces in the xy plane that have a radius greater than 5mm\"\n",
    "f = \"Sort all blends by their z position and then select the last two blends\"\n",
    "g = \"Select all boltholes in the model\"\n",
    "h = \"Select all flanges in the model\"\n",
    "i = \"Select all struts in the model and then select their leading edges\"\n",
    "j = \"Select all strut fillets in the aftmost section of the model\"\n",
    "\n",
    "#First input to the model, breaks query down and provides geometry definition\n",
    "\n",
    "class analyzed_query(BaseModel):\n",
    "    \"\"\"Identifying whether a geometric feature is present in the database and stating the name of the feature\"\"\"\n",
    "    answer: str= Field(description=\"YES or NO\")\n",
    "    features: List= Field(description=\"Name of the identified geometrical feature (Singular)\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Listing the answer of whether the feature is present in the database and the name of the feature\"\"\"\n",
    "    Response: List[analyzed_query]\n",
    "    \n",
    "\n",
    "chat_openai = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
    "chat_google = ChatVertexAI(model_name=\"gemini-1.0-pro\", temperature=0.0, convert_system_message_to_human=False)\n",
    "chat_claude = ChatAnthropic(model_name = \"claude-3-sonnet-20240229\", temperature=0.0) #type: ignore\n",
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''change the human variable to test different inputs'''\n",
    "human = (\"select all leading edges\")\n",
    "\n",
    "def generate_yaml(human:str, chat):\n",
    "    config = {\"metadata\": {\"conversation_id\": str(uuid.uuid4())}}\n",
    "    parser = PydanticOutputParser(pydantic_object=analyzed_query)\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", qa_instructions + \"Answer the user query. Print the output in Json format according to the instructions.\\n{format_instructions}\",), (\"human\", human),]).partial(format_instructions=parser.get_format_instructions())\n",
    "    chain = prompt | chat | parser\n",
    "    query_analyzer = chain.invoke({}, config=config)\n",
    "    answer = query_analyzer.answer\n",
    "\n",
    "    extracted_features = query_analyzer.features\n",
    "    print(query_analyzer)\n",
    "    print()\n",
    "    loader = DirectoryLoader(\"RAG\")\n",
    "    raw_docs = loader.load()\n",
    "    textsplitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "    docs = textsplitter.split_documents(raw_docs)\n",
    "    db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "    retriever = db.as_retriever()\n",
    "    \n",
    "\n",
    "    prompt_for_rag = ChatPromptTemplate.from_template(\"\"\"You are an assistant that is an expert at RAG (Retrieval Augmented Generation). You have been tasked with looking for the definition of the word provided to you as input and generate a summary of the available information.\"\n",
    "                You will use only the context provided to you.                                 \n",
    "                Context:{context}\n",
    "                Query_to_db:{query}\"\"\")\n",
    "    definer = ({\"context\": retriever, \"query\": RunnablePassthrough()}) | prompt_for_rag | chat | StrOutputParser()\n",
    "    #Base prompt chain, generates the first yaml version with minimal context\n",
    "\n",
    "    base_prompt = ChatPromptTemplate.from_messages([(\"system\", system1), few_shot_prompt, MessagesPlaceholder(variable_name=\"messages\")])\n",
    "    base_chain = base_prompt | chat \n",
    "    rag_output = definer.invoke((f\"What is the definition of {extracted_features}?\"), config=config)\n",
    "\n",
    "    #print(rag_output)\n",
    "    #print()\n",
    "\n",
    "    def chain_yes():\n",
    "        chain = base_chain | StrOutputParser()\n",
    "        yaml = chain.invoke({\"messages\": [human]},config=config)\n",
    "        #print(\"RAG not required, generating YAML\")\n",
    "        #print()\n",
    "       #print(yaml)\n",
    "        #print()\n",
    "        return yaml\n",
    "\n",
    "    def chain_no():\n",
    "        chain = base_chain | StrOutputParser()\n",
    "        ai = rag_output\n",
    "        yaml = chain.invoke({\"messages\":[ai]}, config=config)\n",
    "        #print(\"Getting more information using RAG, then generating YAML\")\n",
    "        #print()\n",
    "        #print(yaml)\n",
    "        #print()\n",
    "        return yaml\n",
    "\n",
    "    def decision_maker(response_obj: analyzed_query):\n",
    "        for item in response_obj:\n",
    "            if item == \"YES\":\n",
    "                return chain_yes()\n",
    "            elif item == \"NO\":\n",
    "                return chain_no()\n",
    "\n",
    "    model_output = []\n",
    "    for answer in query_analyzer:\n",
    "        model_output.append(decision_maker(answer))\n",
    "    #This chain ise used for the reflection prompt, which asks the model to critique and improve on the generated YAML text.\n",
    "    reflection_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", refsystem),\n",
    "            few_shot_prompt,\n",
    "            (\"human\", rag_output),\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "    reflect = reflection_prompt | chat | StrOutputParser()\n",
    "    message = f\"Generate constructive feedback for the YAML Code: \\n{model_output[0]}, \\nCheck for selection capability and correctness of the code based on the following prompt that was used to generate this code: \\n{human}\"\n",
    "    #print(message)\n",
    "    feedback = reflect.invoke({\"messages\": [AIMessage(content=message)]},config=config)\n",
    "    \n",
    "\n",
    "    final_generate = base_prompt | chat | StrOutputParser()\n",
    "    output_yaml = final_generate.invoke(\n",
    "        {\"messages\": [f\"I have some YAML code \\n {model_output[0]}\" + f\"\\nThis code needs to be regenerated with the following instructions in mind: \\n{feedback}.\\n Make sure to output only YAML and nothing else.\"]}, config=config)\n",
    "    \n",
    "    print(remove_code_fences(output_yaml))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='NO' features=['boltholes']\n",
      "\n",
      "\n",
      "- filter: select * from faces where type like \"Cylindrical\" and radius < 2\n",
      "- filter: select * from faces where normal_xangle between 70 and 110 and normal_yangle between 70 and 110\n",
      "- filter: select * from faces where x < max(x) - 10 or x > min(x) + 10\n"
     ]
    }
   ],
   "source": [
    "human = g\n",
    "generate_yaml(human, chat_google)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
