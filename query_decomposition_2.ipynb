{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 1,
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticToolsParser, PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from typing import Sequence, List\n",
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"Context_files\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    print()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system1 = context_gen(\"system1.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "refsystem = context_gen(\"ref_system copy.txt\")\n",
    "\n",
    "system3 = context_gen(\"system3.txt\")\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "\n",
    "rag_prompt = context_gen(\"rag_prompt.txt\")\n",
    "\n",
    "\n",
    "#First input to the model, breaks query down and provides geometry definition\n",
    "\n",
    "class analyzed_query(BaseModel):\n",
    "    \"\"\"Identifying whether a geometric feature is present in the database and stating the name of the feature\"\"\"\n",
    "    answer: str= Field(description=\"YES or NO\")\n",
    "    features: List= Field(description=\"Name of the identified geometrical feature (Singular)\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Listing the answer of whether the feature is present in the database and the name of the feature\"\"\"\n",
    "    Response: List[analyzed_query]\n",
    "    \n",
    "#chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "#chat = ChatVertexAI(model_name=\"gemini-1.0-pro\", temperature=0.0, convert_system_message_to_human=True)\n",
    "chat = ChatAnthropic(model_name = \"claude-3-sonnet-20240229\", temperature=0.0)\n",
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''change the human variable to test different inputs'''\n",
    "human = (\"Select all faces with radius equal to 5 and then select their corresponding edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=analyzed_query)\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", rag_prompt + \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",), (\"human\", human),]).partial(format_instructions=parser.get_format_instructions())\n",
    "chain = prompt | chat | parser\n",
    "feature_extractor = chain.invoke({})\n",
    "\n",
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO \n",
      " ['leading edges']\n"
     ]
    }
   ],
   "source": [
    "'''change the human variable to test different inputs'''\n",
    "human = (\"select all leading edges\")\n",
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
    "\n",
    "parser = PydanticOutputParser(pydantic_object=analyzed_query)\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", rag_prompt + \"Answer the user query. Print the output in Json format according to the instructions.\\n{format_instructions}\",), (\"human\", human),]).partial(format_instructions=parser.get_format_instructions())\n",
    "chain = prompt | chat | parser\n",
    "feature_extractor = chain.invoke({})\n",
    "answer = feature_extractor.answer\n",
    "\n",
    "extracted_features = feature_extractor.features\n",
    "print(answer, \"\\n\", extracted_features)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 24,
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"RAG\")\n",
    "raw_docs = loader.load()\n",
    "textsplitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "docs = textsplitter.split_documents(raw_docs)\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "retriever = db.as_retriever()\n",
    " \n",
    "\n",
    "prompt_for_rag = ChatPromptTemplate.from_template(\"\"\"You are an assistant that is an expert at RAG (Retrieval Augmented Generation). You have been tasked with looking for the definition of the word provided to you as input and generate a summary of the available information.\"\n",
    "            You will use only the context provided to you.                                 \n",
    "            Context:{context}\n",
    "            Query_to_db:{query}\"\"\")\n",
    "rag_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()}) | prompt_for_rag | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rag_output = rag_chain.invoke(f\"What is the definition of {extracted_features}?\")\\nprint(rag_output)\\n\\ndef chain_yes():\\n    chain = yaml_generator | StrOutputParser()\\n    yaml = chain.invoke({\"messages\": [human]})\\n    print(\"RAG not required, generating YAML\")\\n    print()\\n    print(yaml)\\n    print()\\n    return yaml\\n\\ndef chain_no():\\n    chain = yaml_generator | StrOutputParser()\\n    ai = AIMessage(content=rag_output)\\n    yaml = chain.invoke({\"messages\":[ai]})\\n    print(\"Getting more information using RAG, then generating YAML\")\\n    print()\\n    print(yaml)\\n    print()\\n    return yaml\\n\\ndef decision_maker(response_obj: analyzed_query):\\n    for item in response_obj:\\n        if item == \"YES\":\\n            return chain_yes()\\n        elif item == \"NO\":\\n            return chain_no()\\n\\noutput = []\\nfor answer in feature_extractor:\\n    output.append(decision_maker(answer))\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, leading edges are defined as the foremost edge of an object, especially in aerodynamics. In the context of CAD, leading edges may refer to the edge of a surface that faces the direction of motion or airflow. They can be identified in the edges table based on their orientation relative to the airflow or motion direction.\n"
     ]
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
    }
   ],
   "source": [
    "#Base prompt chain, generates the first yaml version with minimal context\n",
    "\n",
    "base_prompt = ChatPromptTemplate.from_messages([(\"system\", system1), few_shot_prompt, MessagesPlaceholder(variable_name=\"messages\")])\n",
    "yaml_generator = base_prompt | chat \n",
<<<<<<< HEAD
    "\n",
    "\n",
    "\"\"\"rag_output = rag_chain.invoke(f\"What is the definition of {extracted_features}?\")\n",
=======
    "rag_output = rag_chain.invoke(f\"What is the definition of {extracted_features}?\")\n",
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
    "print(rag_output)\n",
    "\n",
    "def chain_yes():\n",
    "    chain = yaml_generator | StrOutputParser()\n",
    "    yaml = chain.invoke({\"messages\": [human]}, {\"tags\": [\"chain_yes\"]})\n",
    "    print(\"RAG not required, generating YAML\")\n",
    "    print()\n",
    "    print(yaml)\n",
    "    print()\n",
    "    return yaml\n",
    "\n",
    "def chain_no():\n",
    "    chain = yaml_generator | StrOutputParser()\n",
    "    ai = AIMessage(content=rag_output)\n",
    "    yaml = chain.invoke({\"messages\":[ai]}, {\"tags\": [\"chain_no\"]})\n",
    "    print(\"Getting more information using RAG, then generating YAML\")\n",
    "    print()\n",
    "    print(yaml)\n",
    "    print()\n",
    "    return yaml\n",
    "\n",
    "def decision_maker(response_obj: analyzed_query):\n",
    "    for item in response_obj:\n",
    "        if item == \"YES\":\n",
    "            return chain_yes()\n",
    "        elif item == \"NO\":\n",
<<<<<<< HEAD
    "            return chain_no()\n",
    "\n",
    "output = []\n",
    "for answer in feature_extractor:\n",
    "    output.append(decision_maker(answer))\n",
    "\n",
    "\"\"\""
=======
    "            return chain_no()\n"
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 26,
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Select all faces with radius less than 10\n",
      "---\n",
      "\n",
      "- filter: select * from faces where radius < 10\n",
      "---\n",
      "\n",
      "- filter: select * from faces where radius < 10\n",
      "---\n",
      "\n",
      "- filter: select * from faces where radius < 10\n",
      "- expand: faceedges\n",
      "- expand: edgebody\n",
=======
      "Getting more information using RAG, then generating YAML\n",
      "\n",
      "- filter: select * from edges where type like \"Linear\" and length < 10\n",
      "- expand: adjacent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_output = []\n",
    "for answer in feature_extractor:\n",
    "    model_output.append(decision_maker(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- filter: select * from edges where type like \"Linear\" and length < 10\n",
      "- expand: adjacent\n",
      "---\n",
      "\n",
      "The provided YAML code is accurate and directly corresponds to the user's query. The filter condition selects edges of type \"Linear\" with a length less than 10, and the expand command further selects adjacent edges. The code is clear and concise, fulfilling the user's request effectively.\n",
      "---\n",
      "\n",
      "- filter: select * from edges where type like \"Linear\" and length < 10\n",
      "- expand: adjacent\n",
      "---\n",
      "\n",
      "The YAML code provided accurately selects edges of type \"Linear\" with a length less than 10 and then expands to select adjacent edges. The code is clear and directly addresses the user's query. Well done!\n",
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This chain ise used for the reflection prompt, which asks the model to critique and improve on the generated YAML text.\n",
<<<<<<< HEAD
    "reflection_prompt = ChatPromptTemplate.from_messages([(\"system\", refsystem), few_shot_prompt, MessagesPlaceholder(variable_name=\"messages\")])\n",
=======
    "reflection_prompt = ChatPromptTemplate.from_messages([(\"system\", refsystem), few_shot_prompt, (\"ai\", f\"The human is asking to {human}, \"+ f\"here is some added context {rag_output}\"), MessagesPlaceholder(variable_name=\"messages\"), ])\n",
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
    "reflect = reflection_prompt | chat \n",
    "\"\"\"\n",
    "final_output = reflect.invoke({\"messages\": [HumanMessage(content=str(output))]})\n",
    "print(final_output.content)\n",
    "\"\"\"\n",
    "async def generation_node(state: Sequence[BaseMessage]):\n",
    "        return await yaml_generator.ainvoke({\"messages\": state}, {\"tags\": [\"loop 004\"]})\n",
    "\n",
    "async def reflection_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "        # Other messages we need to adjust\n",
    "        cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "        # First message is the original user request. We hold it the same for all nodes\n",
    "        translated = [state[0]] + [cls_map[msg.type](content=msg.content) for msg in state[1:]]\n",
    "        res = await reflect.ainvoke({\"messages\": translated})\n",
    "        # We treat the output of this as human feedback for the generator\n",
    "        return HumanMessage(content=res.content)\n",
    "\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
<<<<<<< HEAD
    "builder.set_entry_point(\"generate\")\n",
=======
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.set_entry_point(\"reflect\")\n",
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > 3:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "graph = builder.compile()\n",
    "\n",
<<<<<<< HEAD
    "async for event in graph.astream([HumanMessage(content=human)]):\n",
=======
    "async for event in graph.astream([HumanMessage(content=str(model_output[0]))]):\n",
>>>>>>> 1b124bff807dffeb7191f30bc37fb5cc8e68beff
    "        for key, value in event.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(value.get('content'))\n",
    "                print('---')\n",
    "                print()\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        print(item.get('content'))\n",
    "                        print('---')\n",
    "                        print()\n",
    "                    else:\n",
    "                        print(item.content)\n",
    "                        print('---')\n",
    "                        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
