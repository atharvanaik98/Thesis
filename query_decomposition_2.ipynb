{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from typing import Sequence, List\n",
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredExcelLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import nest_asyncio\n",
    "from langchain import hub\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#function to call a path to the file and read it. \n",
    "def context_gen(file_name):\n",
    "    Folder = \"Context_files\"\n",
    "    here = Path(locals().get('__file__', Folder)).resolve()\n",
    "    parameter = (here / file_name).read_text()\n",
    "    return parameter\n",
    "\n",
    "#remove code fences from the output\n",
    "def remove_code_fences(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith('```')]\n",
    "    lines[0] = lines[0].replace(' -', '-', 1)\n",
    "    print()\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "#invoke and run the model with the given prompt\n",
    "\n",
    "\n",
    "\n",
    "#create inputs to the model, telling it what needs to be done. \n",
    "#provides system message\n",
    "system1 = context_gen(\"system1.txt\")\n",
    "\n",
    "#provides the few shot examples\n",
    "output_examples = context_gen(\"outputex.txt\")\n",
    "\n",
    "#provides the input examples\n",
    "input_examples = context_gen(\"inputex.txt\")\n",
    "\n",
    "#provides the database schema\n",
    "schema = context_gen(\"dataBaseSchema.txt\")\n",
    "\n",
    "refsystem = context_gen(\"ref_system copy.txt\")\n",
    "\n",
    "system3 = context_gen(\"system3.txt\")\n",
    "\n",
    "system2 = context_gen(\"system2.txt\")\n",
    "\n",
    "rag_prompt = context_gen(\"rag_prompt.txt\")\n",
    "\n",
    "class SubQuery(BaseModel):\n",
    "    '''Search for the geometric definition of a feature across the vector database'''\n",
    "\n",
    "    sub_query: str = Field(description=\"The text to be used as a sub-query in the prompt.\")   \n",
    "#First input to the model, breaks query down and provides geometry definition\n",
    "\n",
    "class YamlText(BaseModel):\n",
    "    text: str = Field(SQLQuery = \"Yaml Text printed one line at a time\")\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": input_examples, \"output\": output_examples},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n",
      "\n",
      "In the query, the term \"boltholes\" refers to holes that are typically used for bolts. Although the term \"boltholes\" is not explicitly mentioned in the database schema, it can be inferred that \"boltholes\" are likely represented as edges with a circular shape and a specific radius.\n",
      "\n",
      "The query specifies that the boltholes should have a radius of 5mm and be in the xy plane, which can be translated to edges with a radius of 5mm and lying in the xy plane. The database schema contains information about edges, including their radius and position in the global coordinate system, which can be used to identify edges that meet the criteria specified in the query.\n"
     ]
    }
   ],
   "source": [
    "'''change the human variable to test different inputs'''\n",
    "human = (\"select all boltholes with a radius of 5mm in the xy plane\")\n",
    "\n",
    "query_analyzer = ChatPromptTemplate.from_messages([(\"system\", rag_prompt), (\"human\", human)])\n",
    "\n",
    "chain = query_analyzer | chat \n",
    "rag_output = chain.invoke({}, {\"tags\": [\"loop 001\"]})\n",
    "print(rag_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SubQuery(sub_query='what is the definition of a bolthole?'), SubQuery(sub_query='what is the definition of the xy plane?')]\n"
     ]
    }
   ],
   "source": [
    "#This model sends the user query to the LLM, which then breaks it down into smaller parts if there are words it does not understand\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system2), (\"human\", human),])\n",
    "chat_with_tools = chat.bind_tools([SubQuery])\n",
    "parser = PydanticToolsParser(tools=[SubQuery])\n",
    "query_analyzer = prompt | chat_with_tools | parser \n",
    "output = query_analyzer.invoke({}, {\"tags\": [\"loop 002\"]})\n",
    "\n",
    "\n",
    "strings = str([item for item in output])\n",
    "print(strings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat prompt template for defining the words \n",
    "definer = ChatPromptTemplate.from_messages([(\"system\", system3), (\"human\", strings),])\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, streaming=True)\n",
    "\n",
    "#using string output parser as the output is a string\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = definer | chat \n",
    "definition = chain2.invoke({}, {\"tags\":[\"loop 003\"]})\n",
    "print(definition.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base prompt chain, generates the first yaml version with minimal context\n",
    "base_prompt = ChatPromptTemplate.from_messages([(\"system\", system1), MessagesPlaceholder(variable_name=\"messages\")])\n",
    "base_chain = base_prompt | chat \n",
    "\n",
    "#This chain ise used for the reflection prompt, which asks the model to critique and improve on the generated YAML text.\n",
    "reflection_prompt = ChatPromptTemplate.from_messages([(\"system\", refsystem), few_shot_prompt, (\"ai\", definition.content), MessagesPlaceholder(variable_name=\"messages\"), ])\n",
    "reflect = reflection_prompt | chat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generation_node(state: Sequence[BaseMessage]):\n",
    "    return await base_chain.ainvoke({\"messages\": state}, {\"tags\": [\"loop 004\"]})\n",
    "\n",
    "\n",
    "async def reflection_node(state: Sequence[BaseMessage]) -> List[BaseMessage]:\n",
    "    # Other messages we need to adjust\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    # First message is the original user request. We hold it the same for all nodes\n",
    "    translated = [state[0]] + [cls_map[msg.type](content=msg.content) for msg in state[1:]]\n",
    "    res = await reflect.ainvoke({\"messages\": translated})\n",
    "    # We treat the output of this as human feedback for the generator\n",
    "    return HumanMessage(content=res.content)\n",
    "\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.set_entry_point(\"generate\")\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if len(state) > 5:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "graph = builder.compile()\n",
    "\n",
    "async for event in graph.astream([HumanMessage(content=human)]):\n",
    "    for key, value in event.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(value.get('content'))\n",
    "            print('---')\n",
    "            print()\n",
    "        elif isinstance(value, list):\n",
    "             for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    print(item.get('content'))\n",
    "                    print('---')\n",
    "                    print()\n",
    "                else:\n",
    "                    print(item.content)\n",
    "                    print('---')\n",
    "                    print()\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To select a flange, look for planar surfaces with a relatively large area compared to neighboring surfaces in the faces table. Flanges can be identified by their large area and planar type. Consider using commands like \"select all faces with radius less than 10mm\" to help in the selection process.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"./Context_files\")\n",
    "raw_docs = loader.load()\n",
    "textsplitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "docs = textsplitter.split_documents(raw_docs)\n",
    "db = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "prompt_for_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()} | prompt_for_rag | chat | StrOutputParser())\n",
    "\n",
    "rag_chain.invoke(\"What is the best way to select a flange?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
